{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420fd066-9299-4005-a0fc-5e547f0e9f9a",
   "metadata": {},
   "source": [
    "# Market Data Visulaizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460c162-6e37-421d-b7d3-9a6d69fa3f17",
   "metadata": {},
   "source": [
    "### Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "54295922-90da-4bc5-b5bd-e25cea31bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact\n",
    "\n",
    "# make plots a bit larger\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8060716-5213-4471-b0d5-18155804e0fb",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d638e0-a4e1-4ea5-bcc0-1e8583a831fc",
   "metadata": {},
   "source": [
    "### Step 1.1: Data Location & Available Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "475c2140-6f4c-4fa4-92f5-63eda9ea31b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/raw/aapl_1d_2010-01-01_2025-08-16_yf.csv'),\n",
       " PosixPath('../data/raw/spy_1d_2020-01-01_2025-08-16_yf.csv')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output .csv files in data/raw/..\n",
    "\n",
    "raw_dir = Path(\"../data/raw\")\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)  # ensure folder exists\n",
    "csv_files = sorted([p for p in raw_dir.glob(\"*.csv\")])\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in ../data/raw yet. Save one with fetch_data_yf.py first.\")\n",
    "csv_files[:5]  # preview list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa771d-f97d-400c-95c2-9a50d5bb704b",
   "metadata": {},
   "source": [
    "### Step 1.2: Choose & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4251f936-d536-43c5-bfd4-ff0643c6e0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd17f9163c54e7fbab4ebb6d6142f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Primary', options=(('AAPL  |  1d  |  2010-01-01 → 2025-08-16  |  aapl_1d_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Easy loader: pick 2 CSVs from ../data/raw with widgets (robust names) ---\n",
    "\n",
    "import re, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "\n",
    "# 1) list candidates\n",
    "files = sorted(glob.glob(str(RAW_DIR / \"*_yf.csv\")))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No *_yf.csv files found in {RAW_DIR}. Run your fetcher first.\")\n",
    "\n",
    "# 2) parse meta from filename\n",
    "pat = re.compile(\n",
    "    r\"(?P<sym>[a-z0-9_\\-]+)_(?P<intv>1[dwhm]|1wk|1mo|[0-9]+[mhdw])(?:_(?P<start>\\d{4}-\\d{2}-\\d{2})_(?P<end>\\d{4}-\\d{2}-\\d{2}))?_yf\\.csv$\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "def parse_meta(path: str):\n",
    "    m = pat.search(Path(path).name)\n",
    "    if not m:\n",
    "        return None\n",
    "    return {\n",
    "        \"symbol\": m.group(\"sym\").upper(),\n",
    "        \"interval\": m.group(\"intv\"),\n",
    "        \"start\": m.group(\"start\"),\n",
    "        \"end\": m.group(\"end\"),\n",
    "        \"name\": Path(path).name,\n",
    "        \"path\": path,\n",
    "    }\n",
    "\n",
    "def label_for(path: str) -> str:\n",
    "    meta = parse_meta(path)\n",
    "    if meta:\n",
    "        s = meta[\"start\"] or \"?\"\n",
    "        e = meta[\"end\"] or \"?\"\n",
    "        return f\"{meta['symbol']}  |  {meta['interval']}  |  {s} → {e}  |  {meta['name']}\"\n",
    "    return f\"(unknown) | {Path(path).name}\"\n",
    "\n",
    "options = [(label_for(p), p) for p in files]\n",
    "\n",
    "# 3) widgets\n",
    "primary_dd = widgets.Dropdown(options=options, description=\"Primary\")\n",
    "bench_dd   = widgets.Dropdown(options=[(\"None (no benchmark)\", \"\")] + options, description=\"Benchmark\")\n",
    "load_btn   = widgets.Button(description=\"Load\", button_style=\"primary\")\n",
    "out        = widgets.Output()\n",
    "\n",
    "display(widgets.VBox([primary_dd, bench_dd, load_btn, out]))\n",
    "\n",
    "# 4) loader helpers\n",
    "def load_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]).dt.tz_localize(None)\n",
    "    df = df.sort_values(\"timestamp\").drop_duplicates(\"timestamp\").reset_index(drop=True)\n",
    "    if \"Adj Close\" in df.columns and not df[\"Adj Close\"].isna().all():\n",
    "        df[\"close\"] = df[\"Adj Close\"]\n",
    "    return df\n",
    "\n",
    "# 5) click handler (uses unambiguous variable names)\n",
    "def on_load(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        global primary_df, primary_symbol, primary_interval, primary_meta\n",
    "        global bench_df, bench_symbol, bench_interval, bench_meta\n",
    "        # (optional) compatibility mirrors:\n",
    "        global df, symbol, interval\n",
    "\n",
    "        # primary\n",
    "        p_path = primary_dd.value\n",
    "        primary_meta = parse_meta(p_path) or {\"symbol\":\"UNKNOWN\",\"interval\":\"(unknown)\",\"name\":Path(p_path).name}\n",
    "        primary_symbol  = primary_meta[\"symbol\"]\n",
    "        primary_interval= primary_meta[\"interval\"]\n",
    "        primary_df      = load_csv(p_path)\n",
    "\n",
    "        print(f\"Primary: {primary_meta['name']}\")\n",
    "        print(f\"  rows={len(primary_df)}  range={primary_df['timestamp'].min().date()} → {primary_df['timestamp'].max().date()}  interval={primary_interval}\")\n",
    "\n",
    "        # (compatibility) mirror to df/symbol/interval so older cells still work\n",
    "        df = primary_df\n",
    "        symbol = primary_symbol\n",
    "        interval = primary_interval\n",
    "\n",
    "        # benchmark (optional)\n",
    "        bench_df = None\n",
    "        bench_symbol = None\n",
    "        bench_interval = None\n",
    "        bench_meta = None\n",
    "\n",
    "        if bench_dd.value:\n",
    "            b_path = bench_dd.value\n",
    "            bench_meta = parse_meta(b_path) or {\"symbol\":\"UNKNOWN\",\"interval\":\"(unknown)\",\"name\":Path(b_path).name}\n",
    "            bench_symbol   = bench_meta[\"symbol\"]\n",
    "            bench_interval = bench_meta[\"interval\"]\n",
    "            bench_df       = load_csv(b_path)\n",
    "\n",
    "            print(f\"Benchmark: {bench_meta['name']}\")\n",
    "            print(f\"  rows={len(bench_df)}  range={bench_df['timestamp'].min().date()} → {bench_df['timestamp'].max().date()}  interval={bench_interval}\")\n",
    "\n",
    "            if (primary_interval != bench_interval\n",
    "                and \"(unknown)\" not in (primary_interval, bench_interval)):\n",
    "                print(f\"⚠️ Intervals differ (primary={primary_interval}, benchmark={bench_interval}). \"\n",
    "                      \"Overlays will use date overlap only.\")\n",
    "\n",
    "            # report overlap\n",
    "            overlap = primary_df[[\"timestamp\"]].merge(bench_df[[\"timestamp\"]], on=\"timestamp\", how=\"inner\")\n",
    "            if overlap.empty:\n",
    "                print(\"⚠️ No overlapping dates between primary and benchmark.\")\n",
    "            else:\n",
    "                print(f\"Overlap: {overlap['timestamp'].min().date()} → {overlap['timestamp'].max().date()}  ({len(overlap)} rows)\")\n",
    "\n",
    "        print(\"\\n✅ Loaded. Variables:\")\n",
    "        print(\"   primary_df, primary_symbol, primary_interval\")\n",
    "        print(\"   bench_df,   bench_symbol,   bench_interval (None if not chosen)\")\n",
    "        print(\"   (compat) df, symbol, interval\\n\")\n",
    "\n",
    "load_btn.on_click(on_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c119c4-ca6f-452b-9303-85934f1f82f8",
   "metadata": {},
   "source": [
    "## Part 2: Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843f49c-9b02-4d0b-9164-9f5222291b27",
   "metadata": {},
   "source": [
    "### Step 2.1: Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "93402445-173b-4abe-8170-cde015645d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust OHLCV sanity check (order-safe & beginner-friendly) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sanity_check_prices(\n",
    "    df: pd.DataFrame,\n",
    "    name: str = \"asset\",\n",
    "    *,\n",
    "    interval: str | None = None,          # e.g., \"1d\", \"1h\" (optional; helps with heuristics)\n",
    "    first_rows: int = 3,                   # scan first N rows for odd gaps\n",
    "    open_close_pct_thresh: float = 0.08,   # 8% tolerance (higher by default for daily equities)\n",
    "    adj_close_scale_tol: float = 0.10,     # 10% tolerance between Close and Adj Close scales\n",
    "    echo_missing_breakdown: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints human-readable QA for an OHLCV DataFrame.\n",
    "    - Verifies required columns, monotonic timestamps, NaNs/dupes.\n",
    "    - Checks 'high >= low', non-negative prices, non-zero-only volume.\n",
    "    - Flags unusually large Open vs Close gaps in the first few rows.\n",
    "    - Warns if 'Adj Close' is on a different scale than 'close' (bad pipeline).\n",
    "    \"\"\"\n",
    "    msgs: list[str] = []\n",
    "\n",
    "    # 0) Required columns present?\n",
    "    required = {\"timestamp\", \"open\", \"high\", \"low\", \"close\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        msgs.append(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # 1) Timestamp sanity\n",
    "    if \"timestamp\" in df.columns:\n",
    "        if not np.issubdtype(df[\"timestamp\"].dtype, np.datetime64):\n",
    "            msgs.append(\"timestamp is not datetime64; consider parse_dates=['timestamp'] on read.\")\n",
    "        if df[\"timestamp\"].isna().any():\n",
    "            msgs.append(\"NaNs in timestamp.\")\n",
    "        dupes = int(df[\"timestamp\"].duplicated().sum())\n",
    "        if dupes > 0:\n",
    "            msgs.append(f\"{dupes} duplicated timestamps.\")\n",
    "        if not df[\"timestamp\"].is_monotonic_increasing:\n",
    "            msgs.append(\"timestamps not strictly increasing (unsorted and/or duplicates present).\")\n",
    "\n",
    "    # 2) NaNs in OHLCV\n",
    "    ohlcv_cols = [c for c in [\"open\", \"high\", \"low\", \"close\", \"volume\"] if c in df.columns]\n",
    "    nan_counts = df[ohlcv_cols].isna().sum()\n",
    "    if nan_counts.sum() > 0:\n",
    "        msgs.append(f\"NaNs in OHLCV: {nan_counts.to_dict()}\")\n",
    "\n",
    "    # 3) Price ordering / sign checks\n",
    "    if {\"high\", \"low\"} <= set(df.columns):\n",
    "        bad = int((df[\"high\"] < df[\"low\"]).sum())\n",
    "        if bad > 0:\n",
    "            msgs.append(f\"{bad} rows where high < low (corrupt).\")\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        if c in df.columns:\n",
    "            neg = int((df[c] < 0).sum())\n",
    "            if neg > 0:\n",
    "                msgs.append(f\"{neg} negative values in '{c}' (invalid for prices).\")\n",
    "\n",
    "    # 4) Large Open vs Close gaps on the first few rows\n",
    "    #    Daily bars often have real overnight gaps. Use a *percentage* threshold and a higher default (8%).\n",
    "    if {\"open\", \"close\"} <= set(df.columns) and len(df) > 0:\n",
    "        n = min(first_rows, len(df))\n",
    "        oc = (df[\"open\"].head(n) - df[\"close\"].head(n)).abs() / df[\"close\"].head(n).replace(0, np.nan)\n",
    "        if oc.dropna().gt(open_close_pct_thresh).any():\n",
    "            pct = 100 * open_close_pct_thresh\n",
    "            hint = \"overnight gaps are common on daily bars\" if interval == \"1d\" else \"check file alignment\"\n",
    "            msgs.append(f\"Large open/close gap (>~{pct:.1f}%) in first {n} row(s) — {hint}.\")\n",
    "\n",
    "    # 5) Volume sanity\n",
    "    if \"volume\" in df.columns:\n",
    "        if df[\"volume\"].fillna(0).sum() == 0:\n",
    "            msgs.append(\"All volumes are zero (suspicious for stocks; normal for some crypto sources).\")\n",
    "        if (df[\"volume\"] < 0).any():\n",
    "            msgs.append(\"Negative volume values present (invalid).\")\n",
    "\n",
    "    # 6) Adj Close vs Close scale check\n",
    "    if \"Adj Close\" in df.columns and \"close\" in df.columns and not df[\"Adj Close\"].isna().all():\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            ratio = (df[\"Adj Close\"] / df[\"close\"]).replace([np.inf, -np.inf], np.nan).median()\n",
    "        if pd.notna(ratio) and abs(ratio - 1.0) > adj_close_scale_tol:\n",
    "            msgs.append(f\"'Adj Close' scale differs from 'close' (median ratio ≈ {ratio:.3f}). \"\n",
    "                        \"You may be mixing adjusted and unadjusted prices.\")\n",
    "\n",
    "    # 7) Print results\n",
    "    header = name.upper()\n",
    "    if msgs:\n",
    "        print(f\"⚠️  {header}: sanity warnings:\")\n",
    "        for m in msgs:\n",
    "            print(\"   -\", m)\n",
    "    else:\n",
    "        print(f\"✅ {header}: sanity check passed.\")\n",
    "\n",
    "    # Summary context\n",
    "    cols = list(df.columns)\n",
    "    print(f\"columns: {cols}\")\n",
    "    if \"timestamp\" in df.columns:\n",
    "        try:\n",
    "            print(f\"range:   {df['timestamp'].min().date()} → {df['timestamp'].max().date()}  | rows: {len(df)}\")\n",
    "        except Exception:\n",
    "            print(f\"rows: {len(df)}\")\n",
    "    if echo_missing_breakdown and ohlcv_cols:\n",
    "        print(\"missing values per column:\")\n",
    "        print(df.reindex(columns=ohlcv_cols).isna().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "468f0f2e-46fd-4929-a91f-fb4e79db0a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  AAPL: sanity warnings:\n",
      "   - Large open/close gap (>~8.0%) in first 3 row(s) — overnight gaps are common on daily bars.\n",
      "columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
      "range:   2010-01-04 → 2025-08-15  | rows: 3929\n",
      "missing values per column:\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check_prices(primary_df, name=primary_symbol, interval=primary_interval)\n",
    "if bench_df is not None:\n",
    "    sanity_check_prices(bench_df, name=bench_symbol, interval=bench_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2242f-f6f5-4029-99d6-36ce1560a13c",
   "metadata": {},
   "source": [
    "### Step 2.2: Closing Price Chart\n",
    "\n",
    "The closing price is the last traded price of the asset for each interval (e.g., daily close).\n",
    "Plotting it over time gives an immediate sense of:\n",
    "\n",
    "\t• Whether the asset is trending upward, downward, or moving sideways.\n",
    "\t• Periods of sharp rises or crashes (e.g., during market events).\n",
    "\t• Long-term growth patterns versus short-term fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9b17d915-52e4-479c-96e7-70a133ead8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3b4a43ceb243af8ceac59e00330af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Overlay benchmark'), Checkbox(value=False, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Closing price plot (primary + optional benchmark with fair comparison) ---\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "def _align_on_dates(df1, df2, name_a=\"a\", name_b=\"b\"):\n",
    "    merged = (\n",
    "        df1[[\"timestamp\",\"close\"]].rename(columns={\"close\": f\"close_{name_a}\"})\n",
    "        .merge(\n",
    "            df2[[\"timestamp\",\"close\"]].rename(columns={\"close\": f\"close_{name_b}\"}),\n",
    "            on=\"timestamp\", how=\"inner\"\n",
    "        )\n",
    "        .sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    )\n",
    "    return merged.dropna()\n",
    "\n",
    "_have_bench = (\"bench_df\" in globals()) and (bench_df is not None)\n",
    "\n",
    "@interact(\n",
    "    show_bench = widgets.Checkbox(value=_have_bench, description=\"Overlay benchmark\"),\n",
    "    normalize  = widgets.Checkbox(value=False,       description=\"Normalize (rebase to 100)\"),\n",
    "    log_scale  = widgets.Checkbox(value=False,       description=\"Log scale (y)\")\n",
    ")\n",
    "def plot_close(show_bench, normalize, log_scale):\n",
    "    if \"df\" not in globals() or df is None or df.empty:\n",
    "        print(\"⚠️ Primary DataFrame `df` is missing or empty.\")\n",
    "        return\n",
    "\n",
    "    sym   = symbol.upper() if \"symbol\" in globals() else \"ASSET\"\n",
    "    bench = bench_symbol.upper() if (\"bench_symbol\" in globals() and bench_df is not None) else None\n",
    "    iv    = f\" ({interval})\" if \"interval\" in globals() else \"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    title = f\"{sym} price{iv}\"\n",
    "\n",
    "    if show_bench and _have_bench and bench:\n",
    "        merged = _align_on_dates(df, bench_df, name_a=\"p\", name_b=\"b\")\n",
    "\n",
    "        if merged.empty:\n",
    "            print(\"⚠️ No overlapping dates with benchmark — cannot overlay.\")\n",
    "            ts, vals = df[\"timestamp\"], df[\"close\"].astype(float).ffill().bfill()\n",
    "            # Solo stats (return & CAGR)\n",
    "            years = (ts.iloc[-1] - ts.iloc[0]).days / 365.25\n",
    "            total_ret = vals.iloc[-1] / vals.iloc[0] - 1 if vals.iloc[0] != 0 else np.nan\n",
    "            cagr = (1 + total_ret)**(1/years) - 1 if years > 0 and np.isfinite(total_ret) else np.nan\n",
    "\n",
    "            if normalize:\n",
    "                vals = 100.0 * vals / vals.iloc[0]\n",
    "                ax.set_ylabel(\"index (100 = start)\")\n",
    "            else:\n",
    "                ax.set_ylabel(\"price\")\n",
    "\n",
    "            ax.plot(ts, vals, lw=2, label=f\"{sym} ({total_ret:+.1%}, CAGR {cagr:.2%})\")\n",
    "        else:\n",
    "            ts = merged[\"timestamp\"]\n",
    "            p  = merged[\"close_p\"].astype(float).to_numpy()\n",
    "            b  = merged[\"close_b\"].astype(float).to_numpy()\n",
    "\n",
    "            # returns & CAGR on overlap\n",
    "            p_ret = p[-1]/p[0] - 1 if p[0] != 0 else np.nan\n",
    "            b_ret = b[-1]/b[0] - 1 if b[0] != 0 else np.nan\n",
    "            years = (ts.iloc[-1] - ts.iloc[0]).days / 365.25\n",
    "            p_cagr = (p[-1]/p[0])**(1/years) - 1 if years > 0 and p[0] > 0 else np.nan\n",
    "            b_cagr = (b[-1]/b[0])**(1/years) - 1 if years > 0 and b[0] > 0 else np.nan\n",
    "\n",
    "            if normalize:\n",
    "                p = 100.0 * p / p[0]\n",
    "                b = 100.0 * b / b[0]\n",
    "                ax.set_ylabel(\"index (100 at start of overlap)\")\n",
    "            else:\n",
    "                ax.set_ylabel(\"price\")\n",
    "\n",
    "            ax.plot(ts, p, lw=2, label=f\"{sym} ({p_ret:+.1%}, CAGR {p_cagr:.2%})\")\n",
    "            ax.plot(ts, b, lw=1.6, alpha=0.95, label=f\"{bench} ({b_ret:+.1%}, CAGR {b_cagr:.2%})\")\n",
    "\n",
    "            title += f\" — vs {bench}\"\n",
    "            ax.text(0.01, 0.01,\n",
    "                    f\"Overlap: {ts.iloc[0].date()} → {ts.iloc[-1].date()}\",\n",
    "                    transform=ax.transAxes, fontsize=9, color=\"gray\")\n",
    "    else:\n",
    "        # Primary only (still show return & CAGR)\n",
    "        ts   = df[\"timestamp\"]\n",
    "        vals = df[\"close\"].astype(float).ffill().bfill()\n",
    "\n",
    "        years = (ts.iloc[-1] - ts.iloc[0]).days / 365.25\n",
    "        total_ret = vals.iloc[-1] / vals.iloc[0] - 1 if vals.iloc[0] != 0 else np.nan\n",
    "        cagr = (1 + total_ret)**(1/years) - 1 if years > 0 and np.isfinite(total_ret) else np.nan\n",
    "\n",
    "        if normalize:\n",
    "            vals = 100.0 * vals / vals.iloc[0]\n",
    "            ax.set_ylabel(\"index (100 = start)\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"price\")\n",
    "\n",
    "        ax.plot(ts, vals, lw=2, label=f\"{sym} ({total_ret:+.1%}, CAGR {cagr:.2%})\")\n",
    "\n",
    "    # Cosmetics\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"date\")\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.grid(alpha=0.3, linestyle=\"--\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    fig.autofmt_xdate(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b020b-8c19-4009-b50f-cdd29be910ca",
   "metadata": {},
   "source": [
    "### Step 2.3: Trading Volume\n",
    "\n",
    "The volume chart shows how much of the asset is traded over time. This matters because large price moves on high volume indicate stronger conviction (more participants), while moves on low volume can be misleading.\n",
    "\n",
    "What’s shown here:\n",
    "\n",
    "\t• Blue line = raw daily trading volume.\n",
    "\t• Orange line = rolling average of volume (default = 20 bars ≈ one month for daily data).\n",
    "    • X-bar average” means the average over the last X data points. One bar = one row of your dataset (daily = 1 day, hourly = 1 hour, weekly = 1 week). So a  20-bar average on daily data ≈ 1 month of trading.\n",
    "\t• This helps you compare today’s activity to “typical” recent levels.\n",
    "\t• You can adjust the window interactively depending on your data frequency (daily, weekly, hourly).\n",
    "\t• Red circles = top-5 spike days (unusually high activity).\n",
    "\n",
    "How to interpret:\n",
    "\n",
    "\t• Spikes far above the average = event days (earnings, splits, news).\n",
    "\t• Persistent rising average = growing participation.\n",
    "\t• Very low volume = quieter regimes, where signals may be less reliable.\n",
    "\n",
    "Typical ranges for the rolling average:\n",
    "\n",
    "\t• Daily data → 10–30 bars (≈ 2–6 weeks)\n",
    "\t• Weekly data → 4–12 bars (≈ 1–3 months)\n",
    "\t• Hourly data → 24–168 bars (≈ 1–7 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1e87b4e1-cbf5-4fdf-9995-3aa8aeba3deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37b7f3de3ec4a51a38246409b17b9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='Avg window', min=5, step=5), IntSlider(value=5, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_volume(avg_window, topn, show_bench)>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Trading Volume Visualization (bars + rolling averages + spikes, with overlay) ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Sliders + checkbox\n",
    "avg_slider   = widgets.IntSlider(value=20, min=5, max=100, step=5, description=\"Avg window\")\n",
    "topn_slider  = widgets.IntSlider(value=5,  min=3, max=20,  step=1, description=\"Mark top-N\")\n",
    "bench_check  = widgets.Checkbox(value=(bench_df is not None), description=\"Overlay benchmark (overlap only)\")\n",
    "\n",
    "def plot_volume(avg_window, topn, show_bench):\n",
    "    fig, axes = plt.subplots(\n",
    "        2 if (show_bench and bench_df is not None) else 1,\n",
    "        1,\n",
    "        figsize=(12, 8 if (show_bench and bench_df is not None) else 5),\n",
    "        sharex=False\n",
    "    )\n",
    "\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "\n",
    "    # --- Primary asset ---\n",
    "    vol_p = df[\"volume\"] / 1e6\n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(df[\"timestamp\"], vol_p, width=1.0, alpha=0.3, label=f\"{symbol.upper()} volume\")\n",
    "    ax1.plot(df[\"timestamp\"], vol_p.rolling(avg_window).mean(),\n",
    "             lw=2, label=f\"{symbol.upper()} {avg_window}-bar avg\")\n",
    "\n",
    "    top_p = vol_p.nlargest(topn)\n",
    "    ax1.scatter(df.loc[top_p.index, \"timestamp\"], top_p.values,\n",
    "                color=\"red\", s=50, zorder=5, label=f\"{symbol.upper()} top-{topn}\")\n",
    "\n",
    "    ax1.set_title(f\"{symbol.upper()} — Trading volume ({interval})\")\n",
    "    ax1.set_ylabel(\"millions of shares\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "    # --- Benchmark overlay (only if overlap exists) ---\n",
    "    if show_bench and bench_df is not None:\n",
    "        merged = df[[\"timestamp\",\"volume\"]].merge(\n",
    "            bench_df[[\"timestamp\",\"volume\"]], on=\"timestamp\", how=\"inner\", suffixes=(\"_p\",\"_b\")\n",
    "        )\n",
    "        if merged.empty:\n",
    "            print(\"⚠️ No overlapping dates with benchmark.\")\n",
    "        else:\n",
    "            ts = merged[\"timestamp\"]\n",
    "            vol_p = merged[\"volume_p\"] / 1e6\n",
    "            vol_b = merged[\"volume_b\"] / 1e6\n",
    "            ax2 = axes[1]\n",
    "\n",
    "            # Bars\n",
    "            ax2.bar(ts, vol_p, width=1.0, alpha=0.3, label=f\"{symbol.upper()} volume\")\n",
    "            ax2.bar(ts, vol_b, width=1.0, alpha=0.3, label=f\"{bench_symbol.upper()} volume\")\n",
    "\n",
    "            # Rolling averages\n",
    "            ax2.plot(ts, vol_p.rolling(avg_window).mean(),\n",
    "                     lw=2, label=f\"{symbol.upper()} {avg_window}-bar avg\")\n",
    "            ax2.plot(ts, vol_b.rolling(avg_window).mean(),\n",
    "                     lw=2, label=f\"{bench_symbol.upper()} {avg_window}-bar avg\")\n",
    "\n",
    "            # Top spikes\n",
    "            top_p = vol_p.nlargest(topn)\n",
    "            top_b = vol_b.nlargest(topn)\n",
    "            ax2.scatter(ts.iloc[top_p.index], top_p.values,\n",
    "                        color=\"red\", s=40, label=f\"{symbol.upper()} top-{topn}\")\n",
    "            ax2.scatter(ts.iloc[top_b.index], top_b.values,\n",
    "                        color=\"orange\", s=40, label=f\"{bench_symbol.upper()} top-{topn}\")\n",
    "\n",
    "            ax2.set_title(f\"Volume overlay — {symbol.upper()} vs {bench_symbol.upper()}  |  \"\n",
    "                          f\"Overlap: {ts.min().date()} → {ts.max().date()}\")\n",
    "            ax2.set_ylabel(\"millions of shares\")\n",
    "            ax2.set_xlabel(\"date\")\n",
    "            ax2.legend()\n",
    "            ax2.grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "    # Cosmetics for all axes\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "widgets.interact(\n",
    "    plot_volume,\n",
    "    avg_window=avg_slider,\n",
    "    topn=topn_slider,\n",
    "    show_bench=bench_check\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108945c9-c79c-4b68-a19f-ce8190965602",
   "metadata": {},
   "source": [
    "### Step 2.4: Cumulative Returns with Risk Metrics\n",
    "\n",
    "This chart shows how an initial investment would have grown (or shrunk) over time if you bought and held the asset.  \n",
    "It also adds risk insights like **drawdowns** (losses from peaks) and the **CAGR** (average annual growth).\n",
    "\n",
    "What it does:\n",
    "- Start with your chosen initial investment (e.g., $1,000).\n",
    "- Each day, the price moves up or down. We calculate the daily return (% change from yesterday).\n",
    "- Returns are **compounded** — every gain or loss builds on the prior day’s value.\n",
    "- This creates a portfolio curve showing how your money evolves over time.\n",
    "- On top of growth, we shade drawdown periods (the drops from prior peaks) and calculate key stats.\n",
    "\n",
    "Key terms explained:\n",
    "- **Compounded Growth**: Returns multiply over time. Example → +10% then –10% leaves you at –1%, not 0.\n",
    "- **Drawdown**: How much you fall from the latest peak. Big drawdowns = rough ride for investors.\n",
    "- **CAGR (Compound Annual Growth Rate)**: The “smoothed” yearly growth rate that would turn your start value into the final value, as if it grew steadily.\n",
    "- **Max Drawdown**: The worst % drop from a peak during the period — a key risk measure.\n",
    "\n",
    "How to read the chart:\n",
    "- **Purple line** = portfolio growth (buy & hold performance).\n",
    "- **Shaded areas** = drawdowns (losses from peaks).\n",
    "- **Metrics box** (bottom-right) = Final $, total % return, CAGR, and Max Drawdown.\n",
    "- **Steady upward slope** → consistent growth.  \n",
    "- **Sharp drops** → crashes or corrections.  \n",
    "- **Flat periods** → stagnation.  \n",
    "- **Start date slider** → shows how entry timing changes your outcome.  \n",
    "- **Log scale toggle** → makes exponential growth periods easier to compare.\n",
    "\n",
    "Finance note ⚖️:\n",
    "- Using **Close prices** = raw market performance (ignores dividends).  \n",
    "- Using **Adjusted Close** = assumes dividends reinvested and stock splits accounted for → truer “investor returns.”  \n",
    "- That’s why numbers here may differ from Google/Yahoo charts, which usually use Adjusted Close.\n",
    "\n",
    "👉 Example:  \n",
    "If you invested $1,000 in Apple in 2010 and it grew to ~$36,000 by 2025, that’s a +3500% return.  \n",
    "The CAGR would be ~24% per year — meaning, on average, it’s like compounding 24% every year, even though the actual path had big ups and downs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cf7bc38a-1996-4a42-8e44-99edbd722b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917f47af151047cf955933f42d08934f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, description='Invest $', max=20000, min=100, step=100), SelectionSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _get_bench():\n",
    "    \"\"\"Return (bench_df, bench_label) if a usable benchmark is in memory; else (None, None).\"\"\"\n",
    "    bdf = globals().get(\"bench_df\", None)\n",
    "    if isinstance(bdf, pd.DataFrame) and not bdf.empty:\n",
    "        return bdf, globals().get(\"bench_symbol\", \"Benchmark\")\n",
    "    return None, None\n",
    "\n",
    "# Date slider options (built from primary df)\n",
    "date_options = [(ts.strftime(\"%Y-%m-%d\"), ts) for ts in df[\"timestamp\"].unique()]\n",
    "default_date = df[\"timestamp\"].iloc[0]\n",
    "\n",
    "@interact(\n",
    "    invest=widgets.IntSlider(value=1000, min=100, max=20000, step=100, description=\"Invest $\"),\n",
    "    start=widgets.SelectionSlider(options=date_options, value=default_date,\n",
    "                                  description=\"Start\", continuous_update=True),\n",
    "    log_scale=widgets.Checkbox(value=False, description=\"Log scale (y)\"),\n",
    "    show_stats=widgets.Checkbox(value=True, description=\"Show stats\"),\n",
    "    # default reflects what's in memory *right now*\n",
    "    show_bench=widgets.Checkbox(value=(_get_bench()[0] is not None), description=\"Overlay benchmark\")\n",
    ")\n",
    "def plot_cumulative(invest, start, log_scale, show_stats, show_bench):\n",
    "    # Slice\n",
    "    sub = df[df[\"timestamp\"] >= start].copy()\n",
    "    if sub.empty:\n",
    "        print(\"No data from this start date.\")\n",
    "        return\n",
    "\n",
    "    # Returns & compounding\n",
    "    sub[\"ret\"] = sub[\"close\"].pct_change().fillna(0.0)\n",
    "    sub[\"cum\"] = (1.0 + sub[\"ret\"]).cumprod()\n",
    "    sub[\"portfolio\"] = invest * sub[\"cum\"]\n",
    "\n",
    "    # Audit & CAGR\n",
    "    cum_ratio_end = sub[\"close\"].iloc[-1] / sub[\"close\"].iloc[0]\n",
    "    if abs(sub[\"cum\"].iloc[-1] - cum_ratio_end) > 1e-3:\n",
    "        print(\"⚠️ Cumprod and price-ratio disagree. Check gaps/NaNs or whether 'close' is adjusted.\")\n",
    "    years = (sub[\"timestamp\"].iloc[-1] - sub[\"timestamp\"].iloc[0]).days / 365.25\n",
    "    cagr  = cum_ratio_end**(1/years) - 1 if years > 0 else float(\"nan\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(sub[\"timestamp\"], sub[\"portfolio\"], color=\"purple\", lw=2, label=f\"Growth of ${invest:,}\")\n",
    "\n",
    "    # Drawdown shading\n",
    "    runmax   = sub[\"portfolio\"].cummax()\n",
    "    drawdown = sub[\"portfolio\"] / runmax - 1.0\n",
    "    ax.fill_between(sub[\"timestamp\"], sub[\"portfolio\"], runmax,\n",
    "                    where=sub[\"portfolio\"] < runmax, alpha=0.15, label=\"Drawdown\")\n",
    "\n",
    "    # Worst DD band & label (only when stats on)\n",
    "    worst_dd = float(drawdown.min()) if np.isfinite(drawdown.min()) else 0.0\n",
    "    if show_stats and np.isfinite(worst_dd) and worst_dd < 0:\n",
    "        dd_vals = drawdown.values\n",
    "        trough_idx = int(np.argmin(dd_vals))\n",
    "        runmax_vals = runmax.values\n",
    "        peak_idx = int(np.argmax(runmax_vals[:trough_idx+1]))\n",
    "        t0 = sub[\"timestamp\"].iloc[peak_idx]\n",
    "        t1 = sub[\"timestamp\"].iloc[trough_idx]\n",
    "        ax.axvspan(t0, t1, color=\"red\", alpha=0.08, zorder=0)\n",
    "        ax.text(sub[\"timestamp\"].iloc[0], ax.get_ylim()[1]*0.98,\n",
    "                f\"Worst DD: {worst_dd:.1%}  ({t0.date()} → {t1.date()})\",\n",
    "                fontsize=9, color=\"#b04040\", va=\"top\")\n",
    "\n",
    "    # Start / End\n",
    "    start_val = float(sub[\"portfolio\"].iloc[0])\n",
    "    end_val   = float(sub[\"portfolio\"].iloc[-1])\n",
    "    pct_total = (end_val / start_val - 1.0) * 100.0\n",
    "    ax.scatter(sub[\"timestamp\"].iloc[0], start_val, color=\"green\", s=60, zorder=3, label=f\"Start: ${start_val:,.2f}\")\n",
    "    ax.scatter(sub[\"timestamp\"].iloc[-1], end_val,   color=\"red\",   s=60, zorder=3, label=f\"End: ${end_val:,.2f}\")\n",
    "    ax.text(sub[\"timestamp\"].iloc[-1], end_val, f\"  Final: ${end_val:,.2f}  ({pct_total:+.1f}%)\",\n",
    "            va=\"center\", fontsize=10, color=\"red\")\n",
    "\n",
    "    # Benchmark overlay (look up the live global every time)\n",
    "    bench_cagr = None\n",
    "    if show_bench:\n",
    "        bdf, blabel = _get_bench()\n",
    "        if bdf is None:\n",
    "            # Gentle hint on the chart rather than a noisy print\n",
    "            ax.text(0.99, 0.98, \"No benchmark loaded\", transform=ax.transAxes,\n",
    "                    ha=\"right\", va=\"top\", fontsize=9, color=\"gray\")\n",
    "        else:\n",
    "            bsub = bdf[bdf[\"timestamp\"] >= start]\n",
    "            merged = sub[[\"timestamp\",\"close\"]].rename(columns={\"close\":\"asset\"}).merge(\n",
    "                bsub.rename(columns={\"close\":\"bench\"}), on=\"timestamp\", how=\"inner\"\n",
    "            )\n",
    "            if not merged.empty:\n",
    "                asset_ix = merged[\"asset\"] / merged[\"asset\"].iloc[0]\n",
    "                bench_ix = merged[\"bench\"] / merged[\"bench\"].iloc[0]\n",
    "                ax.plot(merged[\"timestamp\"], invest * bench_ix, lw=1.8, alpha=0.9, label=blabel)\n",
    "                if years > 0:\n",
    "                    bench_cagr = bench_ix.iloc[-1]**(1/years) - 1\n",
    "\n",
    "    # Metrics box\n",
    "    if show_stats:\n",
    "        lines = [\n",
    "            f\"Final: ${end_val:,.0f}\",\n",
    "            f\"Total: {pct_total:+.1f}%\",\n",
    "            f\"CAGR:  {cagr:.2%}\",\n",
    "            f\"Max DD: {worst_dd:.1%}\",\n",
    "        ]\n",
    "        if bench_cagr is not None and np.isfinite(bench_cagr):\n",
    "            lines.append(f\"vs {globals().get('bench_symbol','Bench')} CAGR: {(cagr - bench_cagr):+.2%} (Δ)\")\n",
    "        ax.text(0.985, 0.02, \"\\n\".join(lines),\n",
    "                transform=ax.transAxes, ha=\"right\", va=\"bottom\",\n",
    "                fontsize=9, bbox=dict(facecolor=\"white\", alpha=0.85, boxstyle=\"round,pad=0.3\"))\n",
    "\n",
    "    # Cosmetics\n",
    "    title_tag = \"Adj Close\" if (\"Adj Close\" in df.columns and df[\"close\"].equals(df[\"Adj Close\"])) else \"Close\"\n",
    "    ttl = f\"Cumulative Returns (Compounded Growth)\\n{title_tag}; CAGR: {cagr:.2%}\"\n",
    "    if bench_cagr is not None and np.isfinite(bench_cagr):\n",
    "        ttl += f\" | {globals().get('bench_symbol','Bench')} CAGR: {bench_cagr:.2%}\"\n",
    "    ax.set_title(ttl)\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Portfolio Value ($)\")\n",
    "    if log_scale: ax.set_yscale(\"log\")\n",
    "    ax.grid(alpha=0.3, linestyle=\"--\"); ax.legend(loc=\"best\")\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    ax.text(0.01, 0.01, f\"Source: {title_tag} | Rebased from {sub['timestamp'].iloc[0].date()}\",\n",
    "            transform=ax.transAxes, fontsize=9, color=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077d47f-f618-45f5-801a-58ef56c59954",
   "metadata": {},
   "source": [
    "### Step 2.5: Candle Stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388ee0f-741c-472f-996c-6dc97d7fbb04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantAI",
   "language": "python",
   "name": "quantai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
